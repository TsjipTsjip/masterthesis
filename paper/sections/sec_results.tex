% !TEX root = ../Victorvan Herel2025_Thesis.tex

\chapter{Method results}\label{ch:results}

\comment{!UNFINISHED CHAPTER! Here be dragons.}

\section{Copyleft clause detection results}

We consider the results of the copyleft detection experiment first as follows: \\

\textbf{Llama3:8n:} Upon a first run of this model with the given task, it was decided not to continue with the additional runs for this model, as its accuracy was already very low and as such Llama3:8b is not a promising avenue compared to other selected models. The confusion matrix for this run is provided in Table~\ref{tab:llama3-1-confmatrix}. This run scores an accuracy of \textbf{55.17\% (64 / 116)}. While the mistakes made by this model are varied, it is clear the model particularly struggles with false positive detections of a copyleft clause.

\begin{table}[h]
	\caption{Llama3:8b confusion matrix (Run 1/1)}
	\label{tab:llama3-1-confmatrix}
	\centering
	\begin{tabular}{l|ll}
		\hline
		\textbf{LLM \textbackslash OSADL} & Copyleft & Permissive \\ \hline
		Copyleft & 25 & 48 \\
		Permissive & 4 & 39 \\\hline
	\end{tabular}
\end{table}

\textbf{Gemma3:4b:} This model performed better, scoring an average accuracy across runs of 83.62\% (97/116). Its mistakes are also exclusively failures to detect a present copyleft license. The confusion matrix for the majority vote is given in Table~\ref{tab:gemma3-confmatrix}, which scored the same accuracy as the average accuracy. The model is very consistent, only differing in responses across runs for the MPL-1.1 license and the MPL-2.0-no-copyleft-exception license.

\textbf{Average accuracy:} 83.62\% (97 / 116)

\textbf{Majority-of-5 accuracy:} 83.62\% (97 / 116)

\begin{table}[h]
	\caption{Gemma3:4b confusion matrix (Majority vote of 5 runs)}
	\label{tab:gemma3-confmatrix}
	\centering
	\begin{tabular}{l|ll}
		\hline
		\textbf{LLM \textbackslash OSADL} & Copyleft & Permissive \\ \hline
		Copyleft & 10 & 0 \\
		Permissive & 19 & 87 \\\hline
	\end{tabular}
\end{table}

\textbf{Deepseek R1:8b:} This model is very inconsistent with its answers, answering differently across runs for 28 licenses in the dataset. Additionally, this model classifies 2 specific licenses incorrectly on all its runs. We will discuss this rather peculiar result after the listing of direct results. In a majority configuration however, we note that the accuracy increases (107 licenses correctly classified, as opposed to 104 in the best run). This suggests that this model does benefit from a majority vote setup. Notably, this model also does not make any false positive detections.

\textbf{Average accuracy:} 88.62\% (\~ 102.8 / 116)

\textbf{Majority-of-5 accuracy:} 92.24\% (107 / 116)

\begin{table}[h]
	\caption{Deepseek R1:8b confusion matrix (Majority vote of 5 runs)}
	\label{tab:deepseekr1-confmatrix}
	\centering
	\begin{tabular}{l|ll}
		\hline
		\textbf{LLM \textbackslash OSADL} & Copyleft & Permissive \\ \hline
		Copyleft & 20 & 0 \\
		Permissive & 9 & 87 \\\hline
	\end{tabular}
\end{table}

- Qwen3:8b: This model is the best model of the selected models for this task. It however doesn't really benefit from the majority vote decision setup, like we have seen for the Deepseek model. Instead, its runs generally are very accurate, outclassing all other 3 models that were included in this experiment, however the best run actually performs better than the majority vote, correctly classifying 113 out of 116 licenses. We see in the confusion matrix that this model does make false positive classifications, as opposed to the other two models which do not make this mistake.

\textbf{Average accuracy:} 96.21\% (\~ 111.6 / 116)

\textbf{Majority-of-5 accuracy:} 96.55\% (112 / 116)

\begin{table}[h]
	\caption{Qwen3:8b confusion matrix (Majority vote of 5 runs)}
	\label{tab:qwen3-confmatrix}
	\centering
	\begin{tabular}{l|ll}
		\hline
		\textbf{LLM \textbackslash OSADL} & Copyleft & Permissive \\ \hline
		Copyleft & 26 & 1 \\
		Permissive & 3 & 86 \\\hline
	\end{tabular}
\end{table}

\section{Examining mistakes made by models}

\subsection{General mistakes}

The Gemma3, Deepseek R1 and Qwen3 model all failed to classify two licenses correctly, looking into the thinking of the models when making this decision, we can explain why:
\begin{itemize}
	\item \textbf{IPL-1.0:} The IBM Public License is a complex case because it handles source code redistribution and binary object redistribution entirely differently. We examine specifically its Section 3 Requirements. This mentions the following:
	\begin{itemize}
		\item "When the Program is made available in source code form: a. it must be made available under this Agreement; and b. a copy of this Agreement must be included with each copy of the Program." This section tells us that source code redistribution is strongly copyleft, requiring the same license is used for the derivative work created.
		\item "A contributor may choose to distribute the Program in object code form under its own license agreement, provided that: [...]" This section governs binary delivery, and it does impose that whichever license agreement you use, it complies with the IPL-1.0, and must disclaim warranty on behalf of the contributors, excluding them from liabilities, damages, ... much like the MIT license. It goes on to permit the new license to differ, but that such different clauses are only offered by the redistributing contributor alone.
		This difference between handling of source code redistribution and binary object redistribution seems to confuse our models. The option to redistribute under a different license, given specific requirements, is not a copyleft clause. As such, the models decide to classify this license as permissive in all runs.
	\end{itemize}
	
	\item \textbf{Sleepycat:} The sleepycat license is also always classified incorrectly. In this case, the source of confusion is very obvious. The sleepycat license itself is actually 3 licenses which were concatenated together. Because of this, and the predictable nature of the format of these licenses, the models get confused when running inference, treating each license referenced as a separate question. As a result, it fails to comply with the answer format provided, meaning automatic detection of the answer fails as well for Deepseek R1 and Qwen3. For Gemma3, it actually is capable of considering the entire license as its own block, however it is mistaken in handling of the license itself, missing the fact that the Sleepycat section of this license is indeed BSD-3 Clause, but adds an additional clause which imposes a Copyleft clause to this format.
\end{itemize}

\subsection{Individual mistakes}

An interesting mistake only made by the Qwen3 model occurs when examining the Artistic-2.0 license. Gemma3 and Deepseek R1 remain faultless here. This mistake seems to stem from Clause 4a in the license, which \textit{is} a copyleft license, but is preceeded by a construct that indicates this is optional if compliance is instead ensured with clause 4b or clause 4c in the license, which are not copyleft clauses. In a sense, Qwen3 correctly answers the question posed, as this simply queries the model for a Yes/No answer to the question: "Does this license contain a copyleft clause?". This differs from being a part of the copyleft license family.

Other mistakes do not appear to form a coherent pattern. Additionally, among the list of most used licenses according to GitHub's Innovation Graph project, mistakes still get made by Gemma3 and Deepseek R1 within this subset. Qwen3 remains faultless within this subset across all of the recorded runs~\cite{github_innovation_graph_2025}.

This list of frequently used licenses is included in this document as follows in order of rank provided: MIT, Apache-2.0, GPL-3.0-only, GPL-3.0-or-later, AGPL-3.0-only, AGPL-3.0-or-later, BSD-3-Clause, GPL-2.0-only, CC0-1.0, Unlicense, MPL-2.0, BSD-2-Clause, CC-BY-4.0, LGPL-3.0-only, LGPL-3.0-or-later, CC-BY-SA-4.0, ISC, MIT-0, BSD-3-Clause-Clear, EPL-2.0, WTFPL, EUPL-1.2, 0BSD, BSL-1.0, Zlib, EPL-1.0, MulanPSL-2.0, UPL-1.0, OFL-1.1, Artistic-2.0

It should be noted that the dataset of OSADL-evaluated licenses does not contain all of the above mentioned licenses. In particular, it does not contain: CC0-1.0, CC-BY-4.0, CC-BY-SA-4.0, BSD-3-Clause-Clear, MulanPSL-2.0, OFL-1.1

\textbf{Impact of this finding:} We conclude by considering the impact of this finding. In particular, this finding is interesting as we can already eliminate a lot of possible combinations with this result. We can do this because OSADL's definition of Compatibility between licenses contains a specification that two licenses which do not contain a copyleft clause are compatible. Likewise, OSADL's definition of Incompatibility stipulates that two licenses containing a copyleft clause without presence of an explicit compatibility clause in the license text.

As an experiment, this allows us for the list of popular licenses above to immediately classify 156 possible license combinations as compatible correctly. It also allows us, ignoring explicit compatibility clauses, to classify 94 of the possible license combinations as incompatible. The actual number is 70 however, as this algorithm is not aware of explicit compatibility clauses. This calculation assumes a total of 576 combinations examined, of which 24 are combinations of the same license, in which compatibility is also assumed.


