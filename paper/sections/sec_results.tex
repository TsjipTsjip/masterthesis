% !TEX root = ../Victorvan Herel2025_Thesis.tex

\chapter{Results}\label{ch:results}

\section{Experiment 1: Copyleft clause}

\subsection{Determining X for Majority-of-X}

As discussed in Chapter~\ref{ch:method}, we will first determine the value of X for a Majority-of-X answer construction. To do this, we need need to define values for X, based on where we observe the stochasticity to have stabilised. We will do this by making observations up to the value of X = 15, where a requirement for further runs would lower the credibility of the model. \\

After running the models for multiple times, we can render the following accuracy table of the first 15 Majority-of-X answers for each LLM. It is important to understand that ordering of individual runs is always preserved while calculating this answer. As such, evening out the stochastic behavior is fair and not based on arbitrary decisions. \\

% Autogenerated file: Copyleft rolling X accuracy table, percentage
\begin{table}[h]
	\label{tab:copyleft-rollingx-perc}
	\centering
	\input{autogen/copyleft_majority_accuracies_rollingx_perc.tex}
	\caption{Accuracy scores for copyleft detection, rolling value of X for Majority-of-X results}
\end{table}

Or in absolute numbers, out of a total of 114 licenses which were given a Copyleft categorization by the OSADL, the following table shows: \\

% Autogenerated file: Copyleft rolling X accuracy table, absolute values
\begin{table}[h]
	\label{tab:copyleft-rollingx-absolute}
	\centering
	\input{autogen/copyleft_majority_accuracies_rollingx_absolute.tex}
	\caption{Absolute number of accurate results for copyleft detection, rolling value of X for Majority-of-X results}
\end{table}

Let's now examine these results individually per model. \\

\textbf{deepseek-r1:8b:} This model appears as the second strongest model for this task in the list. It is also the second most varying model, given that the amount of results it scores correctly varies at 4 points throughout the evolution from Majority-of-1 to Majority-of-15. In terms of absolute change, the most significant change happens at 3 to 5. After this, any change remains within the bounds of 2. At X=7 and onwards, any changes remain within the bounds of 1. For this reason, we choose \textbf{7 for the value of X for deepseek-r1:8b}. \\

\textbf{gemma3:4b:} The gemma3 model appears very stable, already stabilising fully at X = 3. Due to this consistency, we choose \textbf{3 for the value of X for gemma3:4b}. \\

\textbf{llama3:} A varying model which offers great speed and relatively strong accuracy, while among the lower scores in the results. Unlike the other models, this model does not reach a stable point within the first Majority-of-15 result sets. We choose \textbf{15 for the value of X for llama3}, as it is a fast model where making multiple runs is not an issue. \\

\textbf{qwen3:8b:} This model is the strongest on the list, while also having the longest average runtime. We see that for this model, only two points of change occur in the entire table, and this model is therefore very consistent, like gemma3:4b discussed before. Given the stochastic nature of LLM's, choosing value 1 is not appropriate. For this reason, we choose \textbf{3 for the value of X for qwen3:8b}.

\subsection{Accuracy scoring}

We consider the results of the copyleft detection experiment first as follows: \\

\textbf{Deepseek R1:8b:} This model is very inconsistent with its answers, answering differently across runs for 28 licenses in the dataset. Additionally, this model classifies 2 specific licenses incorrectly on all its runs. We will discuss this rather peculiar result after the listing of direct results. In a majority configuration however, we note that the accuracy increases (107 licenses correctly classified, as opposed to 104 in the best run). This suggests that this model does benefit from a majority vote setup. Notably, this model also does not make any false positive detections.

\textbf{Average accuracy:} 89.10\%

\textbf{Majority-of-7 accuracy:} 92.98\% (106/114)

\comment{Serge: Confusion table is not explained.}

\begin{table}[h]
	\label{tab:deepseekr1-confmatrix}
	\centering
	\input{autogen/copyleft_summary_deepseek-r1-8b_majority_of_7.tex}
	\caption{Deepseek R1:8b confusion matrix (Majority-of-7)}
\end{table}

\textbf{Gemma3:4b:} This model performed better, scoring an average accuracy across runs of 83.62\% (97/116). Its mistakes are also exclusively failures to detect a present copyleft license. The confusion matrix for the majority vote is given in Table~\ref{tab:gemma3-confmatrix}, which scored the same accuracy as the average accuracy. The model is very consistent, only differing in responses across runs for the MPL-1.1 license and the MPL-2.0-no-copyleft-exception license.

\textbf{Average accuracy:} 83.33\%

\textbf{Majority-of-3 accuracy:} 83.33\% (95/114)

\begin{table}[h]
	\label{tab:gemma3-confmatrix}
	\centering
	\input{autogen/copyleft_summary_gemma3-4b_majority_of_3.tex}
	\caption{Gemma3:4b confusion matrix (Majority-of-3)}
\end{table}

\textbf{Llama3:} This model performs slightly better than Gemma3:4b, and is also slightly more varied. It provides a fast baseline of what simple models can achieve, allowing you to trade a minor amount of accuracy for faster runtime. However, this model is varied and requires more than 15 runs to become stable, as such, answers provided by this model should be considered as largely stochastic still.

\textbf{Average accuracy:} 83.74\%

\textbf{Majority-of-15 accuracy:} 86.84\% (99/114)

\begin{table}[h]
	\label{tab:llama3-confmatrix}
	\centering
	\input{autogen/copyleft_summary_llama3_majority_of_15.tex}
	\caption{Llama3:8b confusion matrix (Majority-of-15)}
\end{table}

\textbf{Qwen3:8b:} This model is the best model of the selected models for this task. It however doesn't really benefit from the majority vote decision setup, like we have seen for the Deepseek model. Instead, its runs generally are very accurate, outclassing all other 3 models that were included in this experiment, we also observe here that the model actually has a disadvantage when it is placed in a smaller majority configuration like Majority-of-3, as it is susceptible to the first runs that it is fed.

\textbf{Average accuracy:} 96.49\%

\textbf{Majority-of-3 accuracy:} 96.49\% (110/114)

\begin{table}[h]
	\label{tab:qwen3-confmatrix}
	\centering
	\input{autogen/copyleft_summary_qwen3-8b_majority_of_3.tex}
	\caption{Qwen3:8b confusion matrix (Majority-of-3)}
\end{table}

\subsection{General conlusions}

These accuracy characteristics display an interesting property: The accuracy of a given model in a majority configuration will continue rising until it stabilises around a given point. This is a logical consequence of the fact that, while an answer for an individual license is deterministic, it will produce an average accuracy for that individual license as more runs are added to the data in consideration. As our accuracy measure is an aggregate of those for individual runs, this measure will display the same property.

It is then a logical conclusion to say that a model will, generally, answer wrongly for a subset of the input licenses. And in this avenue, we examine whether or not we can identify any reasons for these mistakes made by the models. We start by examining the two licenses which all models got consistently wrong.

\subsubsection{Universal mistakes}

The Gemma3, Deepseek R1 and Qwen3 model all failed to classify two licenses correctly, looking into the thinking of the models when making this decision, we can explain why:
\begin{itemize}
	\item \textbf{IPL-1.0:} The IBM Public License is a complex case because it handles source code redistribution and binary object redistribution entirely differently. We examine specifically its Section 3 Requirements. This mentions the following:
	\begin{itemize}
		\item "When the Program is made available in source code form: a. it must be made available under this Agreement; and b. a copy of this Agreement must be included with each copy of the Program." This section tells us that source code redistribution is strongly copyleft, requiring the same license is used for the derivative work created.
		\item "A contributor may choose to distribute the Program in object code form under its own license agreement, provided that: [...]" This section governs binary delivery, and it does impose that whichever license agreement you use, it complies with the IPL-1.0, and must disclaim warranty on behalf of the contributors, excluding them from liabilities, damages, ... much like the MIT license. It goes on to permit the new license to differ, but that such different clauses are only offered by the redistributing contributor alone.
		This difference between handling of source code redistribution and binary object redistribution seems to confuse our models. The option to redistribute under a different license, given specific requirements, is not a copyleft clause. As such, the models decide to classify this license as permissive in almost all runs.
	\end{itemize}
	
	\item \textbf{Sleepycat:} The sleepycat license is also always classified incorrectly. In this case, the source of confusion is a bit more obvious. The sleepycat license itself is actually 3 licenses which were concatenated together. Because of this, and the predictable nature of the format of these licenses, the models get confused when running inference, treating each license referenced as a separate question. As a result, it fails to comply with the answer format provided, meaning automatic detection of the answer fails as well for Deepseek R1 and Qwen3. For Gemma3, it actually is capable of considering the entire license as its own block, however it is mistaken in handling of the license itself, missing the fact that the Sleepycat section of this license is indeed BSD-3 Clause, but adds an additional clause which imposes a Copyleft clause to this format.
\end{itemize}

\subsubsection{Individual mistakes}

\comment{Victor: This heading needs a lot more love, and I intend to look at each mistaken license itself, determine where its copyleft clause is or isn't, and based on that, I will explain the model's mistake based on the reasonings it has provided.}

\textbf{Deepseek R1:} This model is a well-performing model that only seems to struggle on licenses which other models aside from qwen3 struggle to classify correctly as well. These are the CDDL and MS license families (4 total, though Deepseek specifically does correctly handle the MS-PL license). In terms of mistakes not shared by other models, it seems to struggle on the CPL-1.0 license, as well as both EPL licenses.

\textbf{Gemma3:} We observe two kinds of mistakes for this model. Firstly, for many of the mistakes, we see that this model has answered incorrectly across the board. This makes this model unique in this sense, as none of the other models examined exhibit this kind of absolute behavior. Unfortunately, it is rather hard to determine why it does this. Secondly, we see that it is not as consistent for the MPL license family (3 total on the list) specifically, where it does answer nondeterministically each time the license is examined.

\textbf{Llama3:} The mistakes of this model are harder to discuss in a general sense and appear to be linked to the model's limited reasoning capacity. We observe that in most cases, the model simply picks out an existing clause in the license and then considers that to be the full reasoning for a given decision. Another pattern it provides in its reasonings is a sort of reasoning loop, where it places the confident statement that the license does or does not contain a copyleft clause, and then argues that by providing the definition of such a clause, without making references to the actual license text.

\textbf{Qwen3:} Artistic-2.0, CPL-1.0


\section{Experiment 2: Combination reasoning}

The experiment as outlined in Chapter~\ref{ch:method} comes down to simply running an algorithm. When we do this, we obtain the following results table, which has the Majority-of-X parameter in the leftmost column. Results in bold are the results which correspond to the Majority-of-X value chosen for the model at the start of this chapter. \\


\begin{table}[h]
	\label{tab:experiment2-large}
	\centering
	\begin{tabular}{|r|c|c|c|c|}
		\hline
		\textbf{X} &  \textbf{Deepseek R1}   &     \textbf{Gemma3}     &     \textbf{Llama3}      &      \textbf{Qwen3}      \\ \hline
		         1 &     86.90\% (10706)     &     79.26\% (9765)      &      80.89\% (9966)      &     94.66\% (11662)      \\
		         3 &     87.15\% (10737)     & \textbf{80.18\% (9878)} &     83.98\% (10346)      & \textbf{93.74\% (11549)} \\
		         5 &     89.43\% (11018)      &     80.19\% (9880)      &     81.18\% (10001)      &     93.74\% (11549)      \\
		         7 & \textbf{90.25\% (11119)} &     80.19\% (9880)      &     82.66\% (10184)      &     94.63\% (11658)      \\
		         9 &     90.97\% (11208)      &     80.19\% (9880)      &     83.57\% (10296)      &     94.63\% (11658)      \\
		        11 &     90.97\% (11208)      &     80.19\% (9880)      &     84.25\% (10380)      &     94.63\% (11658)      \\
		        13 &     90.97\% (11208)      &     80.19\% (9880)      &     85.44\% (10526)      &     94.63\% (11658)      \\
		        15 &     90.04\% (11093)      &     80.19\% (9880)      & \textbf{84.50\% (10411)} &     94.63\% (11658)      \\ \hline
	\end{tabular}
	\caption{Combination reasoning results: Perc\% (Number)}
\end{table}

To make sense of this table, we also list the following observations:
\begin{itemize}
	\item For 116 licenses, a total of $116^2 = 13456$ permutations exist.
	\item 116 of these combinations are combining the license with itself, which is always trivially compatible. These were not included in the results table above.
	\item Of the remaining combinations, the OSADL did not asses 1020. These are also not counted in the results table above.
	\item The remaining number of combinations that were assessed using the proposed assessment algorithm is: $13456 - 116 - 1020 = 12320$
\end{itemize}

An immediately obvious observation is that, despite the complexities outlined in the previous chapters, the proposed automated method of assessing license combinations is able to achieve a relatively high accuracy still overall. However, as we recall, a large number of the license combinations is Permissive-X in the dataset. We've also observed in Experiment 1 that this category is the easiest for the models to classify. For that reason, we also present the following summarized observations when we take out all 9775 cases that have a leading permissive license in the OSADL copyleft table, for the respective X values we've already chosen for each model (which we've placed in the table header next to the model).

\begin{table}[h]
	\label{tab:experiment2-permissive-removed}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Deepseek R1 (7)} & \textbf{Gemma3 (3)} & \textbf{Llama3 (15)} & \textbf{Qwen3 (3)} \\ \hline
		      80.26\%        &     69.16\%     &     75.80\%     &    83.35\%     \\ \hline
	\end{tabular}
	\caption{Non Permissive-X combination reasoning results: Perc\% (Number)}
\end{table}

When we compare Table~\ref{tab:experiment2-large} to Table~\ref{tab:experiment2-permissive-removed}, we can say that all models are indeed worse at classifying models where a copyleft license may be involved. What's interesting is that all models lose approximately the same amount of accuracy too, all around 10\%. This is a qualitative observation, yet it shows that all models we assessed are somewhat similarly affected by the difficulty of copyleft license assessment. It is also important to point out that all models do consistently get the majority of these assessments right. \\

When we assess our four models, it is clear that on all cases, \textbf{Qwen3} performs the best, with \textbf{Deepseek R1} following close behind. The other two models are less accurate, but were observed to be significantly faster, again showing the tradeoff between accuracy and speed of inference. \\

Lastly, we also observe that the \textbf{Gemma3} model is very consistent, showing the same result across all X values. We also observe similar, but less consistent behavior with the \textbf{Deepseek R1} model. These behaviors are a direct consequence of their consistency which we already observed in Experiment 1, as this Experiment is a deterministic extension of those results.

\section{Answering the research questions}

At the end of Chapter~\ref{ch:introduction}, we asked two research questions. This section serves to consolidate observations made in these experiments, and formulate those into a clear answer.

\subsection{Important software license properties}

Individually, the license's copyleft property has been shown to be an incredibly powerful indicator of its interaction patterns with other licenses. Interestingly, our experiment did not make a difference between Copyleft Limited and Strong Copyleft license families, and despite this the models still managed to obtain strong accuracy results. This indicates that the incompatibility interaction between the Permissive and Copyleft families is a lot stronger, as opposed to any internal incompatibility interactions for the latter.

Secondarily, a factor which was not explored by our experiment but can still make inference more accurate is explicit compatibility or incompatibility detection. These often take the shape of an upgrade path which was discussed in Chapter~\ref{ch:related-work}, but as licenses can be custom made, explicit statements work as well. \\

It should be noted however that while these factors can be used by an automated reasoning system to assess license combinations at scale, it cannot achieve full accuracy due to the complex nature of legal documents and interactions of specific words, often subject to human curation in the forms of judges interpreting the law. Like with all computer-aided automated reasoning, if there is no clear algorithm, human curation is still a requirement before these decisions are actually used in the real world.

\subsection{Feasibility assessment}

This thesis also asked the question to what extent large language models can be feasibly deployed to answer these license compatibility questions. This thesis answers this question by means of the combination of Experiment 1 and Experiment 2, which shows that this is indeed largely feasible. In the previous section, we already addressed the high importance of human curation, and this cannot be fully removed out of the equation. However, we conclude that this can still be a valuable assistive tool to provide a baseline which can be curated, which is less human-intensive than having to assess an entire matrix of combinations from scratch. \\

To conclude, we address that this thesis is not the end. Rather, we have found that this niche of applying machine learning on software licensing remains largely unexplored by the academic community. This thesis can serve as a baseline, based on which future work can be formed.