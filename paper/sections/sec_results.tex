% !TEX root = ../Victorvan Herel2025_Thesis.tex

\chapter{Results}\label{ch:results}

\section{PoC 1: Copyleft clause detection results}

\subsection{Determining X for Majority-of-X}

As discussed in Chapter~\ref{ch:method}, we will first determine the value of X for a Majority-of-X answer construction. To do this, we need a border point \comment{Victor: Fill this with requirements for a border point.}. \\

After running the models for multiple times, we can render the following accuracy table of the first 15 Majority-of-X answers for each LLM. It is important to understand that ordering of individual runs is always preserved while calculating this answer. As such, evening out the stochastic behavior is fair and not based on arbitrary decisions. \\

% Autogenerated file: Copyleft rolling X accuracy table, percentage
\begin{table}[h]
	\label{tab:copyleft-rollingx-perc}
	\centering
	\input{autogen/copyleft_majority_accuracies_rollingx_perc.tex}
	\caption{Accuracy scores for copyleft detection, rolling value of X for Majority-of-X results}
\end{table}

Or in absolute numbers, out of a total of 114 licenses which were given a Copyleft categorization by the OSADL, the following table shows: \\

% Autogenerated file: Copyleft rolling X accuracy table, absolute values
\begin{table}[h]
	\label{tab:copyleft-rollingx-absolute}
	\centering
	\input{autogen/copyleft_majority_accuracies_rollingx_absolute.tex}
	\caption{Absolute number of accurate results for copyleft detection, rolling value of X for Majority-of-X results}
\end{table}

Let's now examine these results individually per model. \\

\textbf{deepseek-r1:8b:} This model appears as the second strongest model for this task in the list. It is also the second most varying model, given that the amount of results it scores correctly varies at 4 points throughout the evolution from Majority-of-1 to Majority-of-15. In terms of absolute change, the most significant change happens at 3 to 5. After this, any change remains within the bounds of 2. At X=7 and onwards, any changes remain within the bounds of 1. For this reason, we choose \textbf{7 for the value of X for deepseek-r1:8b}. \\

\textbf{gemma3:4b:} The gemma3 model appears very stable, already stabilising fully at X = 3. Due to this consistency, we choose \textbf{3 for the value of X for gemma3:4b}. \\

\textbf{llama3:} A surprisingly varied model, given that it starts at around 55\% accuracy, but rises all the way up to 86\% accuracy at Majority-of-15. Given that this is the fastest running model, this necessity for more runs is acceptable. We have however not seen the end of the evolution for llama3, as the trend did not stagnate at the boundary 15. For now, we choose \textbf{15 for the value of X for llama3}, but we will make additional runs for this model to see if we can further pinpoint an appropriate border point. \\

\textbf{qwen3:8b:} This model is the strongest on the list, while also having the longest average runtime. We see that for this model, only two points of change occur in the entire table, and this model is therefore very consistent, like gemma3:4b discussed before. Given the stochasitc nature of LLM's, choosing value 1 is not appropriate. For this reason, we choose \textbf{3 for the value of X for qwen3:8b}.

\subsubsection{Finding the border point for Llama3}

Because of the fact that the accuracy scores for Llama3 have not reached a stable point yet, we have made the choice to add more runs for the model.

The results of this work give us the following accuracy graph which has been extended with the additional runs we have generated for this model.

\comment{Victor: Fill this.}

\subsection{Accuracy scoring}

We consider the results of the copyleft detection experiment first as follows: \\

\textbf{Deepseek R1:8b:} This model is very inconsistent with its answers, answering differently across runs for 28 licenses in the dataset. Additionally, this model classifies 2 specific licenses incorrectly on all its runs. We will discuss this rather peculiar result after the listing of direct results. In a majority configuration however, we note that the accuracy increases (107 licenses correctly classified, as opposed to 104 in the best run). This suggests that this model does benefit from a majority vote setup. Notably, this model also does not make any false positive detections.

\textbf{Average accuracy:} 89.10\%

\textbf{Majority-of-7 accuracy:} 92.98\% (106/114)

\begin{table}[h]
	\label{tab:deepseekr1-confmatrix}
	\centering
	\input{autogen/copyleft_summary_deepseek-r1-8b_majority_of_7.tex}
	\caption{Deepseek R1:8b confusion matrix (Majority-of-7)}
\end{table}

\textbf{Gemma3:4b:} This model performed better, scoring an average accuracy across runs of 83.62\% (97/116). Its mistakes are also exclusively failures to detect a present copyleft license. The confusion matrix for the majority vote is given in Table~\ref{tab:gemma3-confmatrix}, which scored the same accuracy as the average accuracy. The model is very consistent, only differing in responses across runs for the MPL-1.1 license and the MPL-2.0-no-copyleft-exception license.

\textbf{Average accuracy:} 83.33\%

\textbf{Majority-of-3 accuracy:} 83.33\% (95/114)

\begin{table}[h]
	\label{tab:gemma3-confmatrix}
	\centering
	\input{autogen/copyleft_summary_gemma3-4b_majority_of_3.tex}
	\caption{Gemma3:4b confusion matrix (Majority-of-3)}
\end{table}

\textbf{Llama3:} This model is, as already mentioned, special as opposed to the other models in the sense that it doesn't perform well at all individually, but when placed in a majority configuration, it surpasses the Gemma3 model. This seems to suggest that this model is prone to making very varied mistakes, but it doesn't make the same mistakes often, allowing a majority configuration to recover the accuracy for an individual result which may be wrong, up to a limit. We also see this in the following datapoints.

\textbf{Average accuracy:} \comment{Victor: TBD}

\textbf{Majority-of-15 accuracy:} \comment{Victor: TBD}

\begin{table}[h]
	\label{tab:llama3-confmatrix}
	\centering
	\input{autogen/copyleft_summary_llama3_majority_of_15.tex}
	\caption{Llama3:8b confusion matrix (Majority-of-15)}
\end{table}

\textbf{Qwen3:8b:} This model is the best model of the selected models for this task. It however doesn't really benefit from the majority vote decision setup, like we have seen for the Deepseek model. Instead, its runs generally are very accurate, outclassing all other 3 models that were included in this experiment, we also observe here that the model actually has a disadvantage when it is placed in a smaller majority configuration like Majority-of-3, as it is susceptible to the first runs that it is fed.

\textbf{Average accuracy:} 96.49\%

\textbf{Majority-of-3 accuracy:} 96.49\% (110/114)

\begin{table}[h]
	\label{tab:qwen3-confmatrix}
	\centering
	\input{autogen/copyleft_summary_qwen3-8b_majority_of_3.tex}
	\caption{Qwen3:8b confusion matrix (Majority-of-3)}
\end{table}

\subsection{General conlusions}

These accuracy characteristics display an interesting property: The accuracy of a given model in a majority configuration will continue rising until it stabilises around a given point. This is a logical consequence of the fact that, while an answer for an individual license is deterministic, it will produce an average accuracy for that individual license as more runs are added to the data in consideration. As our accuracy measure is an aggregate of those for individual runs, this measure will display the same property.

It is then a logical conclusion to say that a model will, generally, answer wrongly for a subset of the input licenses. And in this avenue, we examine whether or not we can identify any reasons for these mistakes made by the models. We start by examining the two licenses which all models got consistently wrong.

\subsubsection{Universal mistakes}

The Gemma3, Deepseek R1 and Qwen3 model all failed to classify two licenses correctly, looking into the thinking of the models when making this decision, we can explain why:
\begin{itemize}
	\item \textbf{IPL-1.0:} The IBM Public License is a complex case because it handles source code redistribution and binary object redistribution entirely differently. We examine specifically its Section 3 Requirements. This mentions the following:
	\begin{itemize}
		\item "When the Program is made available in source code form: a. it must be made available under this Agreement; and b. a copy of this Agreement must be included with each copy of the Program." This section tells us that source code redistribution is strongly copyleft, requiring the same license is used for the derivative work created.
		\item "A contributor may choose to distribute the Program in object code form under its own license agreement, provided that: [...]" This section governs binary delivery, and it does impose that whichever license agreement you use, it complies with the IPL-1.0, and must disclaim warranty on behalf of the contributors, excluding them from liabilities, damages, ... much like the MIT license. It goes on to permit the new license to differ, but that such different clauses are only offered by the redistributing contributor alone.
		This difference between handling of source code redistribution and binary object redistribution seems to confuse our models. The option to redistribute under a different license, given specific requirements, is not a copyleft clause. As such, the models decide to classify this license as permissive in almost all runs.
	\end{itemize}
	
	\item \textbf{Sleepycat:} The sleepycat license is also always classified incorrectly. In this case, the source of confusion is a bit more obvious. The sleepycat license itself is actually 3 licenses which were concatenated together. Because of this, and the predictable nature of the format of these licenses, the models get confused when running inference, treating each license referenced as a separate question. As a result, it fails to comply with the answer format provided, meaning automatic detection of the answer fails as well for Deepseek R1 and Qwen3. For Gemma3, it actually is capable of considering the entire license as its own block, however it is mistaken in handling of the license itself, missing the fact that the Sleepycat section of this license is indeed BSD-3 Clause, but adds an additional clause which imposes a Copyleft clause to this format.
\end{itemize}

\subsubsection{Individual mistakes}

\textbf{Deepseek R1:} This model is a well-performing model that only seems to struggle on licenses which other models aside from qwen3 struggle to classify correctly as well. These are the CDDL and MS license families (4 total, though Deepseek specifically does correctly handle the MS-PL license). In terms of mistakes not shared by other models, it seems to struggle on the CPL-1.0 license, as well as both EPL licenses.

\textbf{Gemma3:} We observe two kinds of mistakes for this model. Firstly, for many of the mistakes, we see that this model has answered incorrectly across the board. This makes this model unique in this sense, as none of the other models examined exhibit this kind of absolute behavior. Unfortunately, it is rather hard to determine why it does this, \comment{Victor: Read the license's explanations}. Secondly, we see that it is not as consistent for the MPL license family (3 total on the list) specifically, where it does answer nondeterministically each time the license is examined. This \comment{Victor: Explain this further too.}.

\textbf{Llama3:} The mistakes of this model are harder to discuss in a general sense and appear to be linked to the model's limited reasoning capacity. We observe that in most cases, the model simply picks out an existing clause in the license and then considers that to be the full reasoning for a given decision. Another pattern it provides in its reasonings is a sort of reasoning loop, where it places the confident statement that the license does or does not contain a copyleft clause, and then argues that by providing the definition of such a clause, without making references to the actual license text.

\textbf{Qwen3:} Artistic-2.0, CPL-1.0

% \section{PoC 2: Explicit (in)compatibility detection}

% \section{PoC 3: Combined reasoning on license combinations}