% !TEX root = ../Victorvan Herel2025_Thesis.tex

\chapter{Method}\label{ch:method}

For this thesis, we will be interfacing with large language models through the Ollama tool. We've selected this tool as we've found it to be very easy to use, providing a simple way to download models and subsequently querying them manually, or through an API which we can use in our reproduction package to automate certain tasks in obtaining our results~\cite{ollama_docs}.

For our methods proposed, we will perform this method on multiple LLM's to gauge their aptitude, and potentially also identify characteristics that an LLM should have to be able to give mostly correct answers. Where possible, we will aim for high decision accuracy on the question: Is a given combination of two licenses, where one is the leading license and the other the subordinate license, generally compatible? We do this to be able to dynamically construct a matrix in the same format of the license compatibility matrix provided by the OSADL.

\subsection{LLM selection}

For this thesis, we examine the LLM's outlined below. A general selection criterion was their availability. If an LLM is not openly available to run offline (with necessary preceeding downloads allowed), it is not the subject of this thesis as the goal is to provide a method that is open, and which anyone coming across the project could run on their own.

Aside from this criterion, we also clarify specific reasons for choosing a specific model as follows:
\begin{itemize}
	\item \textbf{Llama3:} A model which is indicated as being lightweight and general-purpose. This model does not feature a thinking property, which will generally make it faster than other models that do feature this property~\cite{ollama_llama3}. We include these non-thinking models in the proof-of-concept of this thesis to provide an indication of the trade-off between accuracy of prediction and prediction speed.
	\item \textbf{Gemma3:} This model is more recent compared to the Llama3 model, and as such it is a reasonable expectation for this model to perform better. Like Llama3, this model is a non-thinking model which is advertised to be capable of general-purpose language processing tasks~\cite{ollama_gemma3}.
	\item \textbf{Deepseek R1:} Deepseek R1 is the first model with a thinking property on this list. We also note that it is one of the first open models that feature this thinking capability~\cite{ollama_deepseekr1}. Generally, we expect this to be one of the better performing models because of its supposedly expanded reasoning capacity.
	\item \textbf{Qwen3:} The Qwen3 model is the most recently published model in the list of models this thesis uses. Like Deepseek R1, it features a thinking propert. It also advertises enhanced logical reasoning capacities, which are useful given the logical nature of the problems this thesis poses and seeks a solution to~\cite{ollama_qwen3}.
\end{itemize}

\subsection{Handling LLM stochasticity}

By design, large language models provide stochastic responses. This means that not every response generated by a model given an identical input will produce the same output, which provides a challenge to handle these responses automatically. How can we be certain that a model's answer is correct given the fact that it may answer differently given enough times to query the model identically? \\

This thesis will used a method inspired by ensemble classifiers in clustering problems to lessen the effect a stochasticly uncertain response may have on further reasoning based on it.

To do this, we will run each query-model combination multiple times, asking it questions which have a closed set of potential answers as much as possible. Where this isn't possible, such as with producing a list of license names, manual curation is needed to produce this closed answer set. For the example, this would be listing the SPDX license ID's for the license names produced. The final answer that will be regarded as the definitive answer of the model is the majority vote of the model's different responses over the answer sets. \\

A variable to consider is the amount of times a model is presented an identical query for this majority vote system. We will discuss the way this variable is determined inside the discussion of each of the methods, generally basing it on a border point where further querying is not likely to change a significant portion of the answers it provides on a question with a known ground truth\comment{Does there have to be a ground truth for this approach to work?}. We will call this answer the \textbf{Majority-of-X} answer, where X is the number of queries used in determining said answer.

\section{PoC 1: Copyleft}

A first question this thesis aims to provide a proof of concept for is the ability for large language models to discern whether a given license text contains a copyleft clause. We do this as we recall that, in Chapter~\ref{ch:related-work}, we determined that the presence of a copyleft clause, and thus the license family to which a license belongs, is a strong indicator for determining further license configuration compatibilities. \\

With the models selected, we simply query the model for each license to obtain its answer to the question: Does this license text contain a copyleft clause?

This allows us to assign the license to a license family. We also have the ability to check this evaluation against a ground truth provided by the OSADL, which allows us to assess an accuracy score for each model across the entire dataset. It should be noted specifically, that we will \textbf{not} be considering the licenses of which the OSADL provides the answer: Questionable. We will still allow the models to assess these licenses, but for the purposes of calculating accuracy, we don't include these results as there is no ground-truth answer to compare to.\\

In order to address stochastic behavior which is inherent to large language models, we will query each model multiple times per query as indicated in the general notes section of this chapter. To determine the amount of times we will be querying the model for the answer, we will be querying the model a large number of times as part of this Proof of Concept. When done, we can generate accuracy-of-X scores, which is the accuracy of the Majority-of-X answer from the first X answers against the ground truth. We determine a border point in these scores where changes to the accuracy score are no longer significant. \comment{Victor: How?} This border point defines the value of X we will use when discussing the further results of this segment in Chapter~\ref{ch:results}. \\

The query each model is presented with for each run is the following, where the license's text is inserted in the indicated position:

\begin{verbatim}
	=== LICENSE FULL TEXT ===
	
	{license_fulltext}
	
	=== INSTRUCTION ===
	
	You are a license compatibility expert. Does the license contain a copyleft clause?
	A copyleft clause is a provision that requires derivative works to be distributed
	under the same license terms as the original work, ensuring that the freedoms
	granted by the license are preserved in derivative works.
	Begin your answer with a yes or a no for easy parsing.
\end{verbatim}

The license full text used is fetched from the SPDX license database, because of its comprehensive nature and ability to have fetching be fully automated based on SPDX license identifiers~\cite{spdx-licenses}. The SPDX license database is the most comprehensive authority of license texts which are meant for public use. Other tools such as ScanCode and FOSSology use SPDX license identifiers to refer to matched licenses~\cite{scancode-home}\cite{fossology-home}. \\

Lastly we remark that this query poses a yes/no question which limites the LLM to generating binary responses only. This is a required aspect of the \textbf{Majority-of-X} approach, as we need a closed answer set to determine a majority vote. For this proof of concept element, what is the majority is simply the majority vote on the single yes/no answers for a given query.

Lastly, we remark that a yes/no answer differs from the OSADL dataset described in Chapter~\ref{ch:related-work}, which also includes the does make a difference between strict and loosely copyleft. In the same chapter, we clarify that this generally does not make a difference and as such we don't need to further specify the answer. We also do not allow the LLM to produce the answer "Questionable", as to ensure it is able to provide an answer every time, rather than defaulting to the neutral answer for the more complex licenses.

\subsection{Evaluation}

We will evaluate the effectiveness of this method using an accuracy score per model for the Majority-of-X answer. We will also discuss some other accuracy scores as well to indicate specific behaviors in Chapter~\ref{ch:results}.

To do this, we define the following rules for accuracy calculation:

\begin{enumerate}
	\item We say a model is accurate for a given license if its Majority-of-X answer matches that of the ground truth provided by the OSADL. Otherwise, we say the model is inaccurate.
	\item If the OSADL does not provide a ground truth, we do not consider the result in the accuracy score at all.
	\item The total accuracy of the model is expressed as the percentage of licenses for which the model is said to be accurate, as per rule 1.
\end{enumerate}

Given these rules, we will be able to assess and rank all models for their accuracy on existing licenses. These results will be discussed in Chapter~\ref{ch:results}. Based on these results, we are already able to correctly classify a majority of license combinations correctly ($\sim$ 73.276\% at 100\% accuracy of the model). \\

\section{PoC 2: Explicit (in)compatibility}

A second element of this proof of concept involves determining which licenses are listed as explicitly compatible or incompatible. We do this, as this is a necessary element to eliminate some of the CopyleftAny-X license combinations directly.

Once again, we simply provide a query to each model, this time asking the following question: Which licenses are explicitly compatible or incompatible with this license? We do this with the following verbatim query:

\begin{verbatim}
	=== LICENSE FULL TEXT ===
	
	{license_fulltext}
	
	=== INSTRUCTION ===
	
	You are a license compatibility expert. Does the license list any other licenses
	with which it is explicitly compatible or incompatible?
	Answer on two lines, providing a comma-separated list or "NONE" on each.
	The first line holds licenses which are explicitly compatible.
	The last line holds licenses which are explicitly incompatible.
\end{verbatim}

This query produces results which are not binary. The results are also not in a sufficient closed answer set, as a license can have multiple names. For this reason, manual curation of these results is needed to determine a list of license ID's for each answer.

When we have this list, it is sufficiently closed to perform majority voting per license. A license is either in the list, or it is not. If a majority believes it is in the list, the final answer will have that license. If a majority believes it is not, the final answer will follow this answer. \\

\textbf{Important note:} Because of the property presented in Chapter~\ref{ch:related-work} regarding the copyleft and permissive license families, we only need to run this experiment on the subset of licenses that are believed to be copyleft of at least one of the LLM's used by PoC 1.
